# AquaWatch/api-sig/etl_anomalies.py
import asyncio
import asyncpg
import logging
import os

# Configuration des logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Variables d'environnement (√† d√©finir dans docker-compose.yml ou .env)
TIMESCALEDB_DSN = os.getenv(
    "TIMESCALEDB_DSN",
    "postgresql://aquawatch:example@timescaledb:5432/aquawatch"
)
POSTGIS_DSN = os.getenv(
    "POSTGIS_DSN",
    "postgresql://aquawatch:example@postgis:5432/aquawatch_gis"
)

async def sync_anomalies():
    """Synchronise les anomalies de TimescaleDB vers PostGIS (table spatiale)."""
    src = None
    dst = None
    
    try:
        # Connexion aux deux bases
        logger.info("=" * 60)
        logger.info("D√âBUT DE LA SYNCHRONISATION")
        logger.info("=" * 60)
        
        logger.info(f"Connexion √† TimescaleDB: {TIMESCALEDB_DSN.split('@')[1] if '@' in TIMESCALEDB_DSN else 'N/A'}")
        src = await asyncpg.connect(TIMESCALEDB_DSN)
        logger.info("‚úì Connexion √† TimescaleDB r√©ussie")
        
        logger.info(f"Connexion √† PostGIS: {POSTGIS_DSN.split('@')[1] if '@' in POSTGIS_DSN else 'N/A'}")
        dst = await asyncpg.connect(POSTGIS_DSN)
        logger.info("‚úì Connexion √† PostGIS r√©ussie")
        
        # V√©rifier que PostGIS est activ√©
        postgis_enabled = await dst.fetchval("SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'postgis');")
        if not postgis_enabled:
            logger.warning("‚ö† Extension PostGIS non activ√©e, tentative d'activation...")
            try:
                await dst.execute("CREATE EXTENSION IF NOT EXISTS postgis;")
                logger.info("‚úì Extension PostGIS activ√©e")
            except Exception as e:
                logger.error(f"Erreur lors de l'activation de PostGIS: {e}")
        
        # V√©rifier que la table existe, sinon la cr√©er
        table_exists = await dst.fetchval("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = 'anomalies_gis'
            );
        """)
        
        if not table_exists:
            logger.warning("‚ö† Table 'anomalies_gis' n'existe pas, cr√©ation...")
            try:
                await dst.execute("""
                    CREATE TABLE anomalies_gis (
                        id TEXT PRIMARY KEY,
                        type TEXT NOT NULL,
                        timestamp TIMESTAMPTZ NOT NULL,
                        sensor_id TEXT,
                        parameter TEXT,
                        value NUMERIC,
                        message TEXT,
                        geom GEOMETRY(Point, 4326) NOT NULL
                    );
                    CREATE INDEX IF NOT EXISTS idx_anomalies_gis_geom ON anomalies_gis USING GIST (geom);
                    CREATE INDEX IF NOT EXISTS idx_anomalies_gis_timestamp ON anomalies_gis (timestamp DESC);
                """)
                logger.info("‚úì Table anomalies_gis cr√©√©e avec succ√®s")
            except Exception as e:
                logger.error(f"ERREUR lors de la cr√©ation de la table: {e}", exc_info=True)
                return
        else:
            logger.info("‚úì Table anomalies_gis trouv√©e dans PostGIS")
            
            # V√©rifier et ajouter la contrainte PRIMARY KEY si elle n'existe pas
            has_pk = await dst.fetchval("""
                SELECT EXISTS (
                    SELECT 1 FROM pg_constraint 
                    WHERE conrelid = 'anomalies_gis'::regclass 
                    AND contype = 'p'
                );
            """)
            
            if not has_pk:
                logger.warning("‚ö† Pas de PRIMARY KEY sur 'id', ajout de la contrainte...")
                try:
                    # V√©rifier s'il y a des doublons avant d'ajouter la PK
                    duplicates = await dst.fetchval("""
                        SELECT COUNT(*) FROM (
                            SELECT id FROM anomalies_gis GROUP BY id HAVING COUNT(*) > 1
                        ) t;
                    """)
                    
                    if duplicates and duplicates > 0:
                        logger.warning(f"‚ö† {duplicates} doublons trouv√©s, nettoyage...")
                        await dst.execute("""
                            DELETE FROM anomalies_gis a
                            USING anomalies_gis b
                            WHERE a.id = b.id AND a.ctid < b.ctid;
                        """)
                        logger.info("‚úì Doublons supprim√©s")
                    
                    await dst.execute("ALTER TABLE anomalies_gis ADD PRIMARY KEY (id);")
                    logger.info("‚úì PRIMARY KEY ajout√©e sur 'id'")
                except Exception as e:
                    logger.error(f"ERREUR lors de l'ajout de la PRIMARY KEY: {e}", exc_info=True)
                    logger.warning("‚ö† Continuons sans PRIMARY KEY (ON CONFLICT ne fonctionnera pas)")
            else:
                logger.info("‚úì PRIMARY KEY sur 'id' v√©rifi√©e")
        
        # Statistiques dans TimescaleDB
        total_in_tsdb = await src.fetchval("SELECT COUNT(*) FROM anomalies;")
        with_coords = await src.fetchval("""
            SELECT COUNT(*) FROM anomalies 
            WHERE latitude IS NOT NULL AND longitude IS NOT NULL;
        """)
        recent_with_coords = await src.fetchval("""
            SELECT COUNT(*) FROM anomalies 
            WHERE latitude IS NOT NULL 
              AND longitude IS NOT NULL
              AND timestamp > NOW() - INTERVAL '7 days';
        """)
        
        logger.info(f"üìä Statistiques TimescaleDB:")
        logger.info(f"   - Total anomalies: {total_in_tsdb}")
        logger.info(f"   - Avec coordonn√©es: {with_coords}")
        logger.info(f"   - R√©centes (7 jours) avec coordonn√©es: {recent_with_coords}")
        
        # Statistiques dans PostGIS
        total_in_postgis = await dst.fetchval("SELECT COUNT(*) FROM anomalies_gis;")
        logger.info(f"üìä Statistiques PostGIS:")
        logger.info(f"   - Total anomalies: {total_in_postgis}")
        
        # Lire les anomalies non encore synchronis√©es (avec LIMIT pour √©viter de charger trop de donn√©es)
        # On synchronise par batches pour g√©rer de grandes quantit√©s
        BATCH_SIZE = 10000  # Traiter 10000 anomalies √† la fois
        MAX_ANOMALIES = 100000  # Maximum total √† synchroniser par ex√©cution
        
        logger.info("üîç Lecture des anomalies depuis TimescaleDB...")
        logger.info(f"   Configuration: batch_size={BATCH_SIZE}, max_total={MAX_ANOMALIES}")
        
        # Compter les anomalies r√©centes (sans v√©rifier si elles existent d√©j√† - ON CONFLICT g√®re √ßa)
        total_recent = await src.fetchval("""
            SELECT COUNT(*) 
            FROM anomalies
            WHERE latitude IS NOT NULL 
              AND longitude IS NOT NULL
              AND timestamp > NOW() - INTERVAL '7 days'
        """)
        
        logger.info(f"   Anomalies r√©centes (7 jours) avec coordonn√©es: {total_recent}")
        
        if total_recent == 0:
            logger.info("‚úÖ Aucune anomalie r√©cente √† synchroniser.")
            return
        
        # Limiter le nombre total pour cette ex√©cution
        limit = min(total_recent, MAX_ANOMALIES)
        logger.info(f"   Synchronisation de {limit} anomalies maximum cette fois-ci...")
        logger.info(f"   (ON CONFLICT DO NOTHING √©vitera les doublons)")
        
        # Lire par batches - √©viter ORDER BY qui est tr√®s lent sur 9M+ lignes
        # Utiliser une approche simple : lire sans tri, on synchronisera tout progressivement
        all_rows = []
        batch_num = 0
        last_id = None  # Utiliser l'ID pour pagination (plus rapide que timestamp)
        
        logger.info(f"   ‚ö† Note: Lecture sans ORDER BY pour performance (9M+ lignes)")
        logger.info(f"   Les anomalies seront synchronis√©es dans l'ordre de la base")
        
        while len(all_rows) < limit:
            batch_num += 1
            batch_limit = min(BATCH_SIZE, limit - len(all_rows))
            
            logger.info(f"   Lecture du batch {batch_num} (d√©j√† {len(all_rows)}/{limit} lues)...")
            
            try:
                # Utiliser WHERE id > last_id pour pagination efficace (pas besoin de ORDER BY)
                if last_id:
                    batch_rows = await src.fetch("""
                        SELECT id, type, timestamp, sensor_id, parameter, value, message, latitude, longitude
                        FROM anomalies
                        WHERE latitude IS NOT NULL 
                          AND longitude IS NOT NULL
                          AND timestamp > NOW() - INTERVAL '7 days'
                          AND id > $1
                        LIMIT $2
                    """, last_id, batch_limit)
                else:
                    # Premier batch : prendre les premi√®res disponibles
                    batch_rows = await src.fetch("""
                        SELECT id, type, timestamp, sensor_id, parameter, value, message, latitude, longitude
                        FROM anomalies
                        WHERE latitude IS NOT NULL 
                          AND longitude IS NOT NULL
                          AND timestamp > NOW() - INTERVAL '7 days'
                        LIMIT $1
                    """, batch_limit)
                
                if not batch_rows:
                    logger.info(f"   ‚úì Plus d'anomalies √† lire")
                    break
                
                all_rows.extend(batch_rows)
                # Mettre √† jour le dernier ID pour la pagination
                last_id = batch_rows[-1]['id']
                logger.info(f"   ‚úì {len(batch_rows)} anomalies lues dans ce batch (total: {len(all_rows)}/{limit})")
                
                # Si on a moins que le batch size, on a fini
                if len(batch_rows) < BATCH_SIZE:
                    logger.info(f"   ‚úì Toutes les anomalies disponibles ont √©t√© lues")
                    break
                
                # Si on a atteint la limite, arr√™ter
                if len(all_rows) >= limit:
                    break
                    
            except Exception as e:
                logger.error(f"   ‚ùå Erreur lors de la lecture du batch {batch_num}: {e}")
                if len(all_rows) > 0:
                    logger.info(f"   Continuons avec {len(all_rows)} anomalies d√©j√† lues...")
                    break
                else:
                    raise

        if not all_rows:
            logger.warning("‚ö† Aucune anomalie √† synchroniser.")
            return

        logger.info(f"‚úÖ {len(all_rows)} anomalies charg√©es et pr√™tes √† √™tre synchronis√©es.")

        # Ins√©rer dans PostGIS avec g√©om√©trie (traiter par batches pour meilleure performance)
        inserted = 0
        skipped_duplicates = 0  # D√©j√† existantes
        skipped_invalid = 0     # Coordonn√©es invalides
        errors = 0
        
        # Traiter par batches pour √©viter de saturer la m√©moire
        total_batches = (len(all_rows) + BATCH_SIZE - 1) // BATCH_SIZE
        
        for batch_num in range(total_batches):
            batch_start = batch_num * BATCH_SIZE
            batch_end = min(batch_start + BATCH_SIZE, len(all_rows))
            batch = all_rows[batch_start:batch_end]
            
            logger.info(f"   Traitement du batch {batch_num + 1}/{total_batches} ({len(batch)} anomalies)...")
            
            # Filtrer et valider les donn√©es avant insertion
            valid_rows = []
            for r in batch:
                # V√©rifier que les coordonn√©es sont valides
                if r['latitude'] is None or r['longitude'] is None:
                    skipped_invalid += 1
                    continue
                
                # V√©rifier que les coordonn√©es sont dans des plages valides
                if not (-90 <= r['latitude'] <= 90) or not (-180 <= r['longitude'] <= 180):
                    skipped_invalid += 1
                    continue
                
                valid_rows.append(r)
            
            if not valid_rows:
                logger.info(f"     Aucune anomalie valide dans ce batch")
                continue
            
            # Ins√©rer une par une avec gestion d'erreur individuelle
            # (√©vite que les erreurs n'abortent toute la transaction)
            for r in valid_rows:
                try:
                    result = await dst.execute("""
                        INSERT INTO anomalies_gis (id, type, timestamp, sensor_id, parameter, value, message, geom)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, ST_SetSRID(ST_MakePoint($8, $9), 4326))
                        ON CONFLICT (id) DO NOTHING
                    """, r['id'], r['type'], r['timestamp'], r['sensor_id'], r['parameter'],
                       r['value'], r['message'], r['longitude'], r['latitude'])
                    
                    # V√©rifier si une ligne a √©t√© ins√©r√©e
                    if result == "INSERT 0 1":
                        inserted += 1
                        if inserted % 1000 == 0:  # Log tous les 1000 insertions
                            logger.info(f"     {inserted} anomalies ins√©r√©es...")
                    else:
                        skipped_duplicates += 1  # D√©j√† existant (ON CONFLICT DO NOTHING)
                        
                except Exception as e:
                    errors += 1
                    if errors <= 10:  # Afficher les 10 premi√®res erreurs
                        logger.error(f"‚ùå Erreur insertion pour id={r['id']}: {e}")
            
            # Log apr√®s chaque batch
            logger.info(f"     Batch {batch_num + 1}/{total_batches} termin√©: {inserted} ins√©r√©es, {skipped_duplicates} doublons, {skipped_invalid} invalides, {errors} erreurs")

        logger.info("=" * 60)
        logger.info(f"‚úÖ SYNCHRONISATION TERMIN√âE")
        logger.info(f"   - {inserted} anomalies ajout√©es")
        logger.info(f"   - {skipped_duplicates} d√©j√† existantes (doublons ignor√©s)")
        logger.info(f"   - {skipped_invalid} avec coordonn√©es invalides (filtr√©es)")
        logger.info(f"   - {errors} erreurs")
        logger.info("=" * 60)
        
        # V√©rification finale dans PostGIS
        final_count = await dst.fetchval("SELECT COUNT(*) FROM anomalies_gis;")
        logger.info(f"üìä Total final dans PostGIS: {final_count} anomalies")

    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"‚ùå ERREUR CRITIQUE lors de la synchronisation: {e}", exc_info=True)
        logger.error("=" * 60)
        raise
    finally:
        if src:
            await src.close()
            logger.debug("Connexion TimescaleDB ferm√©e")
        if dst:
            await dst.close()
            logger.debug("Connexion PostGIS ferm√©e")


async def main_loop():
    """Boucle principale avec gestion correcte de l'event loop."""
    # Ex√©cuter imm√©diatement au d√©marrage
    logger.info("üöÄ D√©marrage de l'ETL de synchronisation...")
    try:
        await sync_anomalies()
    except Exception as e:
        logger.error(f"Erreur lors de la synchronisation initiale : {e}", exc_info=True)
    
    # Puis ex√©cuter toutes les 5 minutes
    while True:
        logger.info("‚è∞ Attente de 5 minutes avant la prochaine synchronisation...")
        await asyncio.sleep(300)  # 300 secondes = 5 minutes
        
        try:
            await sync_anomalies()
        except Exception as e:
            logger.error(f"Erreur lors de la synchronisation : {e}", exc_info=True)


if __name__ == "__main__":
    # Utiliser asyncio.run() une seule fois pour la boucle principale
    try:
        asyncio.run(main_loop())
    except KeyboardInterrupt:
        logger.info("Arr√™t demand√© par l'utilisateur.")
    except Exception as e:
        logger.error(f"Erreur fatale : {e}", exc_info=True)
